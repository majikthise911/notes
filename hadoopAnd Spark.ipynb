{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dahdoop distributeed file system HDFS\n",
    "# splits the blocks of data into multiple blocks \n",
    "# and distributes them across multiple nodes or computers \n",
    "# in a cluster.\n",
    "# MapReduce\n",
    "# MapReduce is a programming model for processing and generating big data sets with a parallel, distributed algorithm on a cluster.\n",
    "# 128 MB is the default block size for HDFS\n",
    "# if there is a remainder of data, it is stored in a separate block\n",
    "# replication factor is the number of copies of a block that are stored in the cluster and is 3\n",
    "\n",
    "\n",
    "# MapReduce splits the data into chunks and distributes them across the cluster to process them in parallel.\n",
    "# input -> split -> mapper pahse -> shuffle -> reducer phase -> output (run on hadoop cluster)\n",
    "# Yarn is a resource manager for hadoop it processes the jobs and allocates resources to them"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
